{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d006aa82",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18bc1a7",
   "metadata": {},
   "source": [
    "### 1. **Supervised vs. Unsupervised Learning**\n",
    "\n",
    "* **Supervised Learning**: Learning from labeled data to make predictions (e.g., classification, regression)\n",
    "* **Unsupervised Learning**: Learning from unlabeled data to find patterns or groupings (e.g., clustering, dimensionality reduction)\n",
    "* **Semi-supervised and Self-supervised Learning**: Combining labeled and unlabeled data to improve model performance\n",
    "* **Reinforcement Learning**: Learning by interacting with the environment and receiving rewards or penalties\n",
    "\n",
    "### 2. **Data Preprocessing and Cleaning**\n",
    "\n",
    "* **Handling Missing Data**: Imputation strategies (mean, median, interpolation, etc.)\n",
    "* **Data Scaling and Normalization**: Standardization (Z-score) and Min-Max scaling\n",
    "* **Data Encoding**:\n",
    "\n",
    "  * One-Hot Encoding for categorical data\n",
    "  * Label Encoding\n",
    "* **Feature Engineering**: Creating new features from raw data (e.g., polynomial features, interaction terms)\n",
    "* **Outliers Detection and Treatment**: Identifying and dealing with extreme values that may distort the model\n",
    "* **Dealing with Imbalanced Datasets**: Resampling techniques (oversampling, undersampling, SMOTE)\n",
    "\n",
    "### 3. **Data Splitting and Evaluation**\n",
    "\n",
    "* **Training, Validation, and Test Split**: How to partition data for model evaluation\n",
    "* **Cross-Validation**: K-fold cross-validation and leave-one-out cross-validation\n",
    "* **Evaluation Metrics**:\n",
    "\n",
    "  * Regression metrics (Mean Absolute Error, Mean Squared Error, R-squared)\n",
    "  * Classification metrics (Accuracy, Precision, Recall, F1-score, ROC Curve, AUC)\n",
    "  * Confusion Matrix (True Positives, True Negatives, False Positives, False Negatives)\n",
    "\n",
    "### 4. **Overfitting and Underfitting**\n",
    "\n",
    "* **Bias-Variance Trade-off**: Understanding the relationship between model complexity and performance\n",
    "* **Regularization**: L1 (Lasso) and L2 (Ridge) regularization to prevent overfitting\n",
    "* **Early Stopping**: Stopping the training process early to prevent the model from learning noise\n",
    "* **Cross-Validation for Tuning**: Using cross-validation to tune hyperparameters and prevent overfitting\n",
    "\n",
    "### 5. **Bias and Fairness in Machine Learning**\n",
    "\n",
    "* **Bias in Data**: Understanding how biased data can lead to biased models\n",
    "* **Fairness Metrics**: Equal opportunity, demographic parity, and equalized odds\n",
    "* **Ethical Considerations**: Addressing issues like discrimination, privacy concerns, and transparency\n",
    "\n",
    "### 6. **Feature Selection and Dimensionality Reduction**\n",
    "\n",
    "* **Feature Selection**: Choosing the most relevant features for a model (filter, wrapper, and embedded methods)\n",
    "* **Principal Component Analysis (PCA)**: Reducing the dimensionality of data while preserving variance\n",
    "* **t-SNE and UMAP**: Non-linear dimensionality reduction for visualization\n",
    "\n",
    "### 7. **Model Selection and Hyperparameter Tuning**\n",
    "\n",
    "* **Grid Search**: Exhaustive search over a predefined set of hyperparameters\n",
    "* **Random Search**: Randomized search over hyperparameter space\n",
    "* **Bayesian Optimization**: Using probabilistic models to select the most promising hyperparameters\n",
    "* **Automated Machine Learning (AutoML)**: Tools like Auto-sklearn and TPOT that automate model selection and hyperparameter tuning\n",
    "\n",
    "### 8. **Ensemble Methods**\n",
    "\n",
    "* **Bagging**: Combining multiple models to reduce variance (e.g., Random Forests)\n",
    "* **Boosting**: Sequentially building models to correct errors of previous models (e.g., Gradient Boosting, XGBoost, AdaBoost)\n",
    "* **Stacking**: Combining different models (often of different types) for improved performance\n",
    "\n",
    "### 9. **Basic Understanding of ML Frameworks**\n",
    "\n",
    "* Popular ML libraries and frameworks such as:\n",
    "\n",
    "  * **Scikit-learn** (for classical ML algorithms)\n",
    "  * **TensorFlow/PyTorch** (for deep learning)\n",
    "  * **XGBoost/LightGBM** (for gradient boosting)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2179784a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
