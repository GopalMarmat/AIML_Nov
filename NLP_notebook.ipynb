{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28e1e5d4",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "Natural Language processing is method to deal with text data, Perform Text Preprocessing...\n",
    "\n",
    "1. Tokenization\n",
    "2. Stemming\n",
    "3. Lemmatization\n",
    "4. Stopword Removing\n",
    "5. POS (Part of Speech) Tagging\n",
    "6. NER (Name Entity Recognition)\n",
    "7. Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b89e60",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "Tokenization is the process of splitting sentence or paragraph into smaller tokens (word), such as words, subwords, sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b9e715a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokenizer: 25 ['Tokenization', 'is', 'the', 'process', 'of', 'splitting', 'sentence', '.', 'or', 'paragraph', 'into', 'smaller', 'tokens', '(', 'word', ')', '.', 'such', 'as', 'words', ',', 'subwords', ',', 'sentences', '.']\n",
      "Sent Tokenizer 3 ['Tokenization is the process of splitting sentence.', 'or paragraph into smaller tokens (word).', 'such as words, subwords, sentences.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/hp/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/hp/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')   # <-- required in newer NLTK versions\n",
    "\n",
    "text = \"Tokenization is the process of splitting sentence. or paragraph into smaller tokens (word). such as words, subwords, sentences.\"\n",
    "word_tokens= word_tokenize(text)\n",
    "sent_tokens=sent_tokenize(text)\n",
    "print(\"Word Tokenizer:\",len(word_tokens), word_tokens)\n",
    "print(\"Sent Tokenizer\",len(sent_tokens),sent_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c6c7b5",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "Stemming is reduce the words to their root from by chopping suffixes. It may not produce valid word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7392c73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'file', 'easili', 'eaten', 'write']\n"
     ]
    }
   ],
   "source": [
    "# PorterStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer=PorterStemmer()\n",
    "\n",
    "words=['running', 'files','easily','eaten', 'writing']\n",
    "\n",
    "stem_word = [stemmer.stem(word) for word in words]\n",
    "print(stem_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bb3a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'file', 'easili', 'eaten', 'write']\n"
     ]
    }
   ],
   "source": [
    "#Snowball Stemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "snow_stemmer=SnowballStemmer('english')\n",
    "\n",
    "words=['running', 'files','easily','eaten', 'writing']\n",
    "\n",
    "snow_stem_word = [snow_stemmer.stem(word) for word in words]\n",
    "\n",
    "print(snow_stem_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "43da33e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['runn', 'file', 'easily', 'eat', 'writ']\n"
     ]
    }
   ],
   "source": [
    "# Regex Stemmer\n",
    "from nltk.stem import RegexpStemmer\n",
    "\n",
    "#while calling regexpstemmer function we need to write regex expression\n",
    "reg_stemmer=RegexpStemmer('ing$|s$|en$', min=3)\n",
    "\n",
    "words=['running', 'files','easily','eaten', 'writing']\n",
    "\n",
    "reg_stem_word = [reg_stemmer.stem(word) for word in words]\n",
    "print(reg_stem_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf5bf32",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "\n",
    "Lemmatization reduces words to their base or dictionary form (lemma), considering grammar and vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8c10507b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['running', 'file', 'easily', 'eaten', 'writing', 'watched']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/hp/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lematizer = WordNetLemmatizer()\n",
    "\n",
    "words=['running', 'files','easily','eaten', 'writing','watched']\n",
    "\n",
    "lemmatize_words = [lematizer.lemmatize(word) for word in words]\n",
    "print(lemmatize_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ac4f2b",
   "metadata": {},
   "source": [
    "## Stop Word:\n",
    "Removing stopword is important in text-preprocessing, because some word not contributiong much meaning data. So Stopward removes less contributing word from text dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ea68da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token data:   ['This', 'is', 'the', 'NLP', 'method', 'which', 'usefull', 'for', 'the', 'text', 'preprocessing', '.']\n",
      "token lenth: 12\n",
      "Filetere data:   ['NLP', 'method', 'usefull', 'text', 'preprocessing', '.']\n",
      "Filter token Lenth: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/hp/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text_data=\"This is the NLP method which usefull for the text preprocessing.\"\n",
    "\n",
    "text_tokens=word_tokenize(text_data)\n",
    "print(\"token data:  \",text_tokens)\n",
    "print(\"token lenth:\", len(text_tokens))\n",
    "\n",
    "stop_word_fun=set(stopwords.words('english'))\n",
    "\n",
    "filter_word= [word for word in text_tokens if word.lower() not in stop_word_fun]\n",
    "print(\"Filetere data:  \",filter_word)\n",
    "print(\"Filter token Lenth:\", len(filter_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f7f687d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: ['This', 'is', 'an', 'example', 'showing', 'off', 'stopword', 'filtration', '.']\n",
      "After Stopword Removal: ['example', 'showing', 'stopword', 'filtration', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/hp/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "words = word_tokenize(\"This is an example showing off stopword filtration.\")\n",
    "filtered = [w for w in words if w.lower() not in stop_words]\n",
    "\n",
    "print(\"Original:\", words)\n",
    "print(\"After Stopword Removal:\", filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17d7450",
   "metadata": {},
   "source": [
    "### POS (Part of Speech) Tagging:\n",
    "POS tagging is used to categorised text grammatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "768a356e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('This', 'DT'), ('is', 'VBZ'), ('the', 'DT'), ('NLP', 'NNP'), ('method', 'NN'), ('which', 'WDT'), ('usefull', 'NN'), ('for', 'IN'), ('the', 'DT'), ('text', 'NN'), ('preprocessing', 'NN'), ('.', '.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/hp/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/hp/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "text_data=\"This is the NLP method which usefull for the text preprocessing.\"\n",
    "token=word_tokenize(text_data)\n",
    "\n",
    "print(pos_tag(token))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64440f55",
   "metadata": {},
   "source": [
    "1️⃣ DT — Determiner\n",
    "\n",
    "Words: This, the\n",
    "\n",
    "A determiner introduces a noun.\n",
    "\n",
    "It gives information about which one, how many, or whose.\n",
    "\n",
    "Examples:\n",
    "\n",
    "This book\n",
    "\n",
    "The method\n",
    "\n",
    "A car\n",
    "\n",
    "My laptop\n",
    "\n",
    "2️⃣ VBZ — Verb (3rd person singular present)\n",
    "\n",
    "Word: is\n",
    "\n",
    "Verb in present tense\n",
    "\n",
    "Used with he/she/it\n",
    "\n",
    "Examples:\n",
    "\n",
    "He runs\n",
    "\n",
    "She writes\n",
    "\n",
    "It works\n",
    "\n",
    "This is correct\n",
    "\n",
    "3️⃣ NNP — Proper Noun (Singular)\n",
    "\n",
    "Word: NLP\n",
    "\n",
    "Name of a specific person, place, organization, or concept.\n",
    "\n",
    "Always capitalized.\n",
    "\n",
    "Examples:\n",
    "\n",
    "John\n",
    "\n",
    "India\n",
    "\n",
    "Python\n",
    "\n",
    "NLP\n",
    "\n",
    "4️⃣ NN — Noun (Singular or Mass)\n",
    "\n",
    "Words: method, usefull, text, preprocessing\n",
    "\n",
    "General noun (not capitalized).\n",
    "\n",
    "Represents a thing, idea, concept, or object.\n",
    "\n",
    "Examples:\n",
    "\n",
    "method\n",
    "\n",
    "book\n",
    "\n",
    "computer\n",
    "\n",
    "preprocessing\n",
    "\n",
    "⚠️ Important:\n",
    "usefull is tagged as NN because:\n",
    "\n",
    "It is misspelled (correct spelling: useful).\n",
    "\n",
    "The tagger thinks it's a noun due to structure.\n",
    "\n",
    "If spelled correctly, it would likely be tagged as JJ (adjective).\n",
    "\n",
    "5️⃣ WDT — Wh-determiner\n",
    "\n",
    "Word: which\n",
    "\n",
    "Used to introduce a relative clause.\n",
    "\n",
    "Refers back to something mentioned earlier.\n",
    "\n",
    "Example:\n",
    "\n",
    "The book which I bought\n",
    "\n",
    "The method which works best\n",
    "\n",
    "6️⃣ IN — Preposition or Subordinating Conjunction\n",
    "\n",
    "Word: for\n",
    "\n",
    "Shows relationship between words.\n",
    "\n",
    "Often indicates direction, purpose, location, time.\n",
    "\n",
    "Examples:\n",
    "\n",
    "for the exam\n",
    "\n",
    "in the room\n",
    "\n",
    "on the table\n",
    "\n",
    "because of rain\n",
    "\n",
    "7️⃣ . — Punctuation\n",
    "\n",
    "Word: .\n",
    "\n",
    "End of sentence marker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "698680a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/hp/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/hp/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/hp/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('John', 'NNP'), ('is', 'VBZ'), ('learning', 'VBG'), ('NLP', 'NNP'), ('using', 'VBG'), ('Python', 'NNP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "text = word_tokenize(\"John is learning NLP using Python.\")\n",
    "print(nltk.pos_tag(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9626552c",
   "metadata": {},
   "source": [
    "## Word Embedding:\n",
    "Embedding represent text data or words as vectors of real numeric value, To capture semantic meaning of words.\n",
    "\n",
    "### Word Embedding Methods:\n",
    "1. TF-IDF (Term Frequency - Inverse Document Frequency)\n",
    "2. Word2vec (CBow and Skipgram)\n",
    "3. One Hot Encoding\n",
    "4. Bag of word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8ea665",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "TF = (No of repetition of word in sentence/no of word in sentence) = embedding = (1/18) and of = (2/18)\n",
    "IDF = loge(no of sentence/ no of sentence containing the word)\n",
    "\n",
    "### Word2Vec\n",
    "Unlike TF-IDF, Word2Vec creates dense vectors that capture semantic meaning.\n",
    "\n",
    "#### A) CBOW (Continuous Bag of Words)\n",
    "\n",
    "Predicts the target word using surrounding context words.\n",
    "\n",
    "Example:\n",
    "\n",
    "Context: \"I love ___ learning\"\n",
    "\n",
    "Target: machine\n",
    "\n",
    "1. Faster\n",
    "2. Works well for frequent words\n",
    "\n",
    "#### B) Skip-Gram\n",
    "\n",
    "Predicts surrounding context words using the target word.\n",
    "\n",
    "Example:\n",
    "\n",
    "Input: machine\n",
    "\n",
    "Output: love, learning\n",
    "\n",
    "1. Works well for rare words\n",
    "2. Better semantic representation\n",
    "\n",
    "### One Hot Encoding:\n",
    "Each word is represented as a vector of 0s with a single 1 at its index position.\n",
    "\n",
    "Example:\n",
    "\n",
    "\"Bag of word\"\n",
    "\n",
    "Bag  - 1 0 0\n",
    "\n",
    "of   - 0 1 0\n",
    "\n",
    "word - 0 0 1\n",
    "\n",
    "### Bag of Words (BoW)\n",
    "\n",
    "\n",
    "Represents text as word frequency counts (ignores grammar and order).\n",
    "\n",
    "Example:\n",
    "\n",
    "Text:\n",
    "\n",
    "\"I love machine learning\"\n",
    "\"I love coding\"\n",
    "\n",
    "Vocabulary:\n",
    "\n",
    "[\"I\", \"love\", \"machine\", \"learning\", \"coding\"]\n",
    "\n",
    "Vectors:\n",
    "\n",
    "[1,1,1,1,0]\n",
    "[1,1,0,0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab25a45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 32 stored elements and shape (3, 29)>\n",
      "  Coords\tValues\n",
      "  (0, 21)\t0.2919139048919681\n",
      "  (0, 5)\t0.22200804888354075\n",
      "  (0, 20)\t0.5838278097839362\n",
      "  (0, 10)\t0.22200804888354075\n",
      "  (0, 9)\t0.2919139048919681\n",
      "  (0, 27)\t0.2919139048919681\n",
      "  (0, 23)\t0.2919139048919681\n",
      "  (0, 4)\t0.2919139048919681\n",
      "  (0, 19)\t0.22200804888354075\n",
      "  (0, 14)\t0.2919139048919681\n",
      "  (1, 19)\t0.16372097540083433\n",
      "  (1, 3)\t0.2152734077990568\n",
      "  (1, 17)\t0.2152734077990568\n",
      "  (1, 2)\t0.2152734077990568\n",
      "  (1, 13)\t0.2152734077990568\n",
      "  (1, 28)\t0.4305468155981136\n",
      "  (1, 0)\t0.2152734077990568\n",
      "  (1, 26)\t0.2152734077990568\n",
      "  (1, 12)\t0.4305468155981136\n",
      "  (1, 16)\t0.2152734077990568\n",
      "  (1, 11)\t0.2152734077990568\n",
      "  (1, 25)\t0.2152734077990568\n",
      "  (1, 22)\t0.2152734077990568\n",
      "  (1, 1)\t0.2152734077990568\n",
      "  (1, 18)\t0.2152734077990568\n",
      "  (1, 8)\t0.2152734077990568\n",
      "  (2, 5)\t0.3349067026613031\n",
      "  (2, 10)\t0.3349067026613031\n",
      "  (2, 6)\t0.4403620672313486\n",
      "  (2, 7)\t0.4403620672313486\n",
      "  (2, 24)\t0.4403620672313486\n",
      "  (2, 15)\t0.4403620672313486\n",
      "Vocalabary: ['as' 'capture' 'data' 'embedding' 'for' 'is' 'john' 'learning' 'meaning'\n",
      " 'method' 'nlp' 'numeric' 'of' 'or' 'preprocessing' 'python' 'real'\n",
      " 'represent' 'semantic' 'text' 'the' 'this' 'to' 'usefull' 'using' 'value'\n",
      " 'vectors' 'which' 'words']\n",
      "TF-IDF:  <bound method _cs_matrix.toarray of <Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 32 stored elements and shape (3, 29)>>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus=[\n",
    "    \"This is the NLP method which usefull for the text preprocessing.\",\n",
    "    \"Embedding represent text data or words as vectors of real numeric value, To capture semantic meaning of words.\",\n",
    "    \"John is learning NLP using Python.\"\n",
    "]\n",
    "\n",
    "vectorizer=TfidfVectorizer()\n",
    "\n",
    "X =vectorizer.fit_transform(corpus)\n",
    "print(X)\n",
    "print(\"Vocalabary:\", vectorizer.get_feature_names_out())\n",
    "print(\"TF-IDF: \", X.toarray)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623be39d",
   "metadata": {},
   "source": [
    "Task:\n",
    "\n",
    "1. Please understand the definition of each concept and then perform handson. practice Tokenization, Stemming, Lemmatization, Stopword , POS Tagging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3db516a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SkipGram Model Output: \n",
      " [-1.0724545e-03  4.7286271e-04  1.0206699e-02  1.8018546e-02\n",
      " -1.8605899e-02 -1.4233618e-02  1.2917745e-02  1.7945977e-02\n",
      " -1.0030856e-02 -7.5267432e-03  1.4761009e-02 -3.0669428e-03\n",
      " -9.0732267e-03  1.3108104e-02 -9.7203208e-03 -3.6320353e-03\n",
      "  5.7531595e-03  1.9837476e-03 -1.6570430e-02 -1.8897636e-02\n",
      "  1.4623532e-02  1.0140524e-02  1.3515387e-02  1.5257311e-03\n",
      "  1.2701781e-02 -6.8107317e-03 -1.8928028e-03  1.1537147e-02\n",
      " -1.5043275e-02 -7.8722071e-03 -1.5023164e-02 -1.8600845e-03\n",
      "  1.9076237e-02 -1.4638334e-02 -4.6675373e-03 -3.8754821e-03\n",
      "  1.6154874e-02 -1.1861792e-02  9.0324880e-05 -9.5074680e-03\n",
      " -1.9207101e-02  1.0014586e-02 -1.7519170e-02 -8.7836506e-03\n",
      " -7.0199967e-05 -5.9236289e-04 -1.5322480e-02  1.9229487e-02\n",
      "  9.9641159e-03  1.8466286e-02]\n",
      "\n",
      " Cbow Model Output: \n",
      " [ 1.56351421e-02 -1.90203730e-02 -4.11062239e-04  6.93839323e-03\n",
      " -1.87794445e-03  1.67635437e-02  1.80215668e-02  1.30730132e-02\n",
      " -1.42324204e-03  1.54208085e-02 -1.70686692e-02  6.41421322e-03\n",
      " -9.27599426e-03 -1.01779103e-02  7.17923651e-03  1.07406788e-02\n",
      "  1.55390287e-02 -1.15330126e-02  1.48667218e-02  1.32509926e-02\n",
      " -7.41960062e-03 -1.74912829e-02  1.08749345e-02  1.30195115e-02\n",
      " -1.57510047e-03 -1.34197120e-02 -1.41718509e-02 -4.99412045e-03\n",
      "  1.02865072e-02 -7.33047491e-03 -1.87401194e-02  7.65347946e-03\n",
      "  9.76895820e-03 -1.28571270e-02  2.41711619e-03 -4.14975407e-03\n",
      "  4.88066689e-05 -1.97670180e-02  5.38400887e-03 -9.50021297e-03\n",
      "  2.17529293e-03 -3.15244915e-03  4.39334614e-03 -1.57631524e-02\n",
      " -5.43436781e-03  5.32639725e-03  1.06933638e-02 -4.78302967e-03\n",
      " -1.90201886e-02  9.01175756e-03]\n"
     ]
    }
   ],
   "source": [
    "## Word2vec\n",
    "from gensim.models import Word2Vec\n",
    "sentence=[\n",
    "  [\"I\", \"Like\",\"Machine\"],\n",
    "  [\"I\",\"am\",\"Learning\"],\n",
    "  [\"Machine\",\"Coding\"]\n",
    " ]\n",
    "\n",
    "# Skip Gram\n",
    "skipgram_model=Word2Vec(sentences=sentence,vector_size=50,window=2,min_count=1,sg=1)\n",
    "\n",
    "cbow_model=Word2Vec(sentences=sentence,vector_size=50,window=2,min_count=1,sg=0)\n",
    "\n",
    "vectors=skipgram_model.wv[\"Machine\"]\n",
    "print(\"SkipGram Model Output:\",\"\\n\",vectors)\n",
    "\n",
    "vectors_cbow=cbow_model.wv[\"Learning\"]\n",
    "print(\"\\n\",\"Cbow Model Output:\",\"\\n\",vectors_cbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "756a18be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "### One Hot Encoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "words = np.array([[\"I\"],[\"Like\"],[\"Machine\"],[\"Learning\"]])\n",
    "\n",
    "ohe_encoder=OneHotEncoder(sparse_output=False)\n",
    "encoded_word=ohe_encoder.fit_transform(words)\n",
    "print(encoded_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "772d3bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['as' 'capture' 'data' 'embedding' 'for' 'is' 'john' 'learning' 'meaning'\n",
      " 'method' 'nlp' 'numeric' 'of' 'or' 'preprocessing' 'python' 'real'\n",
      " 'represent' 'semantic' 'text' 'the' 'this' 'to' 'usefull' 'using' 'value'\n",
      " 'vectors' 'which' 'words']\n",
      "Bow Matrix: \n",
      " [[0 0 0 0 1 1 0 0 0 1 1 0 0 0 1 0 0 0 0 1 2 1 0 1 0 0 0 1 0]\n",
      " [1 1 1 1 0 0 0 0 1 0 0 1 2 1 0 0 1 1 1 1 0 0 1 0 0 1 1 0 2]\n",
      " [0 0 0 0 0 1 1 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "### Bag Of Word\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bow_corpus=[\n",
    "    \"This is the NLP method which usefull for the text preprocessing.\",\n",
    "    \"Embedding represent text data or words as vectors of real numeric value, To capture semantic meaning of words.\",\n",
    "    \"John is learning NLP using Python.\"\n",
    "]\n",
    "\n",
    "bow_vectorizer=CountVectorizer()\n",
    "X=bow_vectorizer.fit_transform(bow_corpus)\n",
    "\n",
    "print(\"Vocabulary:\", bow_vectorizer.get_feature_names_out())\n",
    "print(\"Bow Matrix: \\n\", X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aafc67b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
